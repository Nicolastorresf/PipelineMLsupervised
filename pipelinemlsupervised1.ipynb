{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Escenario\n",
        "\n",
        "Eres un/a ingeniero/a de datos en una empresa de consultoría de análisis de datos. Tu compañía se enorgullece de poder manejar eficientemente conjuntos de datos enormes (grandes volúmenes de datos). Los científicos de datos en tu oficina necesitan trabajar con diferentes algoritmos y datos en diversos formatos. Aunque son expertos en Aprendizaje Automático (Machine Learning), cuentan contigo para realizar trabajos de ETL (Extracción, Transformación y Carga) y construir pipelines (canalizaciones) de Machine Learning.\n",
        "\n",
        "# Objetivos\n",
        "\n",
        "En esta tarea de 4 partes, deberás:\n",
        "\n",
        "# Parte 1: ETL\n",
        "\n",
        "Cargar un conjunto de datos en formato CSV.\n",
        "Eliminar duplicados, si existen.\n",
        "Eliminar filas con valores nulos, si existen.\n",
        "Realizar transformaciones necesarias.\n",
        "Almacenar los datos limpios en formato Parquet.\n",
        "\n",
        "# Parte 2: Creación de Pipeline de Machine Learning\n",
        "\n",
        "Crear un pipeline de Machine Learning para realizar predicciones.\n",
        "\n",
        "# Parte 3: Evaluación del Modelo\n",
        "\n",
        "Evaluar el modelo utilizando métricas adecuadas.\n",
        "Imprimir el intercepto (término independiente) y los coeficientes del modelo.\n",
        "\n",
        "# Parte 4: Persistencia del Modelo\n",
        "\n",
        "Guardar (preservar) el modelo para su uso futuro en producción.\n",
        "Cargar y verificar el modelo almacenado.\n",
        "Conjuntos de Datos (Datasets)\n",
        "\n",
        "En este laboratorio utilizarás el/los siguiente(s) conjunto(s) de datos:\n",
        "\n",
        "Una versión modificada del conjunto de datos sobre el millaje de automóviles (car mileage dataset). El conjunto de datos original está disponible en: https://archive.ics.uci.edu/ml/datasets/auto+mpg"
      ],
      "metadata": {
        "id": "p3PpAlh_W2kJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-D0K5LnWy5z"
      },
      "outputs": [],
      "source": [
        "#Instalación de PySpark\n",
        "!pip install pyspark==3.5 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importaciones y Configuración Inicial\n",
        "import warnings\n",
        "# Suprimir advertencias\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Importar librerías de PySpark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.pipeline import PipelineModel\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer, StandardScaler\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "import os\n",
        "\n",
        "# Crear la Spark Session para el entorno local de Colab\n",
        "# No se necesita findspark.init()\n",
        "spark = SparkSession.builder.appName(\"MPG Prediction Colab\").getOrCreate()\n",
        "\n",
        "print(\"Spark Session creada exitosamente!\")"
      ],
      "metadata": {
        "id": "jdnyU7munyvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Descargar los Datos\n",
        "# Descargar el archivo de datos\n",
        "!wget -q https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-BD0231EN-Coursera/datasets/mpg-raw.csv\n",
        "\n",
        "# Verificar si el archivo se descargó\n",
        "if os.path.exists(\"mpg-raw.csv\"):\n",
        "    print(\"Archivo mpg-raw.csv descargado correctamente.\")\n",
        "else:\n",
        "    print(\"Error: El archivo mpg-raw.csv no se pudo descargar.\")"
      ],
      "metadata": {
        "id": "fLr19RZWoEA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parte 1- ETL\n",
        "print(\"--- Iniciando Parte 1: ETL ---\")\n",
        "\n",
        "# Task 3: Cargar el CSV en un DataFrame de Spark\n",
        "df = spark.read.csv(\"mpg-raw.csv\", header=True, inferSchema=True)\n",
        "print(\"DataFrame cargado desde CSV.\")\n",
        "df.show(5) # Mostrar algunas filas\n",
        "\n",
        "# Task 4 & 5 (Implícito): Ver schema y conteo por Origen (opcional mostrarlo)\n",
        "df.printSchema()\n",
        "print(\"Conteo inicial por Origen:\")\n",
        "df.groupBy('Origin').count().orderBy('count').show()\n",
        "\n",
        "# Task 6: Contar filas iniciales\n",
        "rowcount1 = df.count()\n",
        "print(f\"Número total de filas iniciales (rowcount1): {rowcount1}\")\n",
        "\n",
        "# Task 7: Eliminar filas duplicadas\n",
        "df = df.dropDuplicates()\n",
        "print(\"Filas duplicadas eliminadas.\")\n",
        "\n",
        "# Task 8: Contar filas después de eliminar duplicados\n",
        "rowcount2 = df.count()\n",
        "print(f\"Número total de filas tras dropDuplicates (rowcount2): {rowcount2}\")\n",
        "\n",
        "# Task 9: Eliminar filas con valores nulos\n",
        "df = df.dropna()\n",
        "print(\"Filas con valores nulos eliminadas (dropna).\")\n",
        "\n",
        "# Task 10: Contar filas después de eliminar nulos\n",
        "rowcount3 = df.count()\n",
        "print(f\"Número total de filas tras dropna (rowcount3): {rowcount3}\")\n",
        "\n",
        "# Task 11: Renombrar columna \"Engine Disp\" a \"Engine_Disp\"\n",
        "# Asegúrate de que el nombre original tenga el espacio\n",
        "df = df.withColumnRenamed(\"Engine Disp\", \"Engine_Disp\")\n",
        "print(\"Columna 'Engine Disp' renombrada a 'Engine_Disp'.\")\n",
        "print(\"Schema después de renombrar:\")\n",
        "df.printSchema() # Verificar cambio de nombre\n",
        "\n",
        "# Task 12: Guardar el DataFrame limpio en formato Parquet\n",
        "parquet_path = \"mpg-cleaned.parquet\"\n",
        "df.write.mode(\"overwrite\").parquet(parquet_path)\n",
        "print(f\"DataFrame limpio guardado en: {parquet_path}\")\n",
        "\n",
        "# Evaluación Parte 1 (Adaptada)\n",
        "print(\"\\n--- Evaluación Parte 1 ---\")\n",
        "print(f\"Total rows = {rowcount1}\")\n",
        "print(f\"Total rows after dropping duplicate rows = {rowcount2}\")\n",
        "print(f\"Total rows after dropping duplicate rows and rows with null values = {rowcount3}\")\n",
        "# Verificar el nombre de columna directamente de las columnas actuales\n",
        "renamed_col_name = df.columns[2] # Asumiendo que sigue siendo la tercera columna\n",
        "print(f\"Renamed column name (columna 3) = {renamed_col_name}\")\n",
        "print(f\"{parquet_path} exists : {os.path.isdir(parquet_path)}\")\n",
        "print(\"--- Fin Parte 1 ---\")"
      ],
      "metadata": {
        "id": "o3iASFTPoMut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parte 2\n",
        "print(\"\\n--- Iniciando Parte 2: Creación del Pipeline ---\")\n",
        "\n",
        "# Task 1: Cargar datos desde Parquet\n",
        "df = spark.read.parquet(parquet_path)\n",
        "rowcount4 = df.count()\n",
        "print(f\"DataFrame cargado desde {parquet_path}, filas: {rowcount4}\")\n",
        "print(\"Mostrando 5 filas y schema del DataFrame limpio:\")\n",
        "df.show(5)\n",
        "df.printSchema()\n",
        "\n",
        "# --- Definir etapas del Pipeline ---\n",
        "\n",
        "# Task 2: StringIndexer para la columna 'Origin'\n",
        "indexer = StringIndexer(inputCol=\"Origin\", outputCol=\"OriginIndex\")\n",
        "print(\"Etapa 1: StringIndexer definida.\")\n",
        "\n",
        "# Task 3: VectorAssembler para las características numéricas\n",
        "feature_cols = ['Cylinders', 'Engine_Disp', 'Horsepower', 'Weight', 'Accelerate', 'Year']\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "print(\"Etapa 2: VectorAssembler definida.\")\n",
        "\n",
        "# Task 4: StandardScaler para escalar características\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "print(\"Etapa 3: StandardScaler definida.\")\n",
        "\n",
        "# Task 5: LinearRegression para predecir 'MPG'\n",
        "lr = LinearRegression(featuresCol=\"scaledFeatures\", labelCol=\"MPG\")\n",
        "print(\"Etapa 4: LinearRegression definida.\")\n",
        "\n",
        "# Task 6: Construir el Pipeline\n",
        "pipeline = Pipeline(stages=[indexer, assembler, scaler, lr])\n",
        "print(\"Pipeline construido con 4 etapas.\")\n",
        "\n",
        "# Task 7: Dividir datos en entrenamiento y prueba (70/30)\n",
        "(trainingData, testingData) = df.randomSplit([0.7, 0.3], seed=42)\n",
        "print(f\"Datos divididos: {trainingData.count()} para entrenamiento, {testingData.count()} para prueba.\")\n",
        "\n",
        "# Task 8: Entrenar (ajustar) el pipeline con los datos de entrenamiento\n",
        "print(\"Entrenando el pipeline...\")\n",
        "pipelineModel = pipeline.fit(trainingData)\n",
        "print(\"Pipeline entrenado exitosamente!\")\n",
        "\n",
        "# Evaluación Parte 2 (Adaptada)\n",
        "print(\"\\n--- Evaluación Parte 2 ---\")\n",
        "print(f\"Total rows loaded = {rowcount4}\")\n",
        "ps = [str(x).split(\"_\")[0] for x in pipeline.getStages()]\n",
        "print(f\"Pipeline Stage 1 = {ps[0]}\")\n",
        "print(f\"Pipeline Stage 2 = {ps[1]}\")\n",
        "print(f\"Pipeline Stage 3 = {ps[2]}\")\n",
        "print(f\"Pipeline Stage 4 = {lr.__class__.__name__}\") # Obtener nombre de la clase\n",
        "print(f\"Label column = {lr.getLabelCol()}\")\n",
        "print(\"--- Fin Parte 2 ---\")\n"
      ],
      "metadata": {
        "id": "cSzZhmEgoeHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parte 3- Evaluación del Modelo\n",
        "\n",
        "print(\"\\n--- Iniciando Parte 3: Evaluación del Modelo ---\")\n",
        "\n",
        "# Task 1: Hacer predicciones en los datos de prueba\n",
        "predictions = pipelineModel.transform(testingData)\n",
        "print(\"Predicciones realizadas en el conjunto de prueba.\")\n",
        "print(\"Mostrando algunas predicciones (MPG real vs. Predicción):\")\n",
        "predictions.select(\"MPG\", \"prediction\").show(5)\n",
        "\n",
        "# --- Calcular Métricas ---\n",
        "\n",
        "# Task 2: Calcular MSE (Mean Squared Error)\n",
        "evaluator_mse = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"MPG\", metricName=\"mse\")\n",
        "mse = evaluator_mse.evaluate(predictions)\n",
        "print(f\"Mean Squared Error (MSE) = {mse}\")\n",
        "\n",
        "# Task 3: Calcular MAE (Mean Absolute Error)\n",
        "evaluator_mae = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"MPG\", metricName=\"mae\")\n",
        "mae = evaluator_mae.evaluate(predictions)\n",
        "print(f\"Mean Absolute Error (MAE) = {mae}\")\n",
        "\n",
        "# Task 4: Calcular R-squared (R2)\n",
        "evaluator_r2 = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"MPG\", metricName=\"r2\")\n",
        "r2 = evaluator_r2.evaluate(predictions)\n",
        "print(f\"R-squared (R2) = {r2}\")\n",
        "\n",
        "# Evaluación Parte 3 (Adaptada)\n",
        "print(\"\\n--- Evaluación Parte 3 ---\")\n",
        "print(f\"Mean Squared Error = {round(mse, 2)}\")\n",
        "print(f\"Mean Absolute Error = {round(mae, 2)}\")\n",
        "print(f\"R Squared = {round(r2, 2)}\")\n",
        "\n",
        "# Obtener el modelo de regresión lineal entrenado del pipeline\n",
        "lrModel = pipelineModel.stages[-1] # La última etapa es el modelo LR\n",
        "print(f\"Intercept = {round(lrModel.intercept, 2)}\")\n",
        "print(\"--- Fin Parte 3 ---\")"
      ],
      "metadata": {
        "id": "hSt6M_FTojXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parte 4\n",
        "\n",
        "print(\"\\n--- Iniciando Parte 4: Persistencia del Modelo ---\")\n",
        "\n",
        "# Task 1: Guardar el modelo del pipeline entrenado\n",
        "model_path = \"Practice_Project_MPG_Model\"\n",
        "pipelineModel.write().overwrite().save(model_path) # Usamos overwrite por si se ejecuta varias veces\n",
        "print(f\"Modelo del Pipeline guardado en: {model_path}\")\n",
        "\n",
        "# Task 2: Cargar el modelo del pipeline desde el disco\n",
        "loadedPipelineModel = PipelineModel.load(model_path)\n",
        "print(f\"Modelo del Pipeline cargado desde: {model_path}\")\n",
        "\n",
        "# Task 3: Usar el modelo cargado para hacer predicciones (verificación)\n",
        "print(\"Verificando predicciones con el modelo cargado...\")\n",
        "loaded_predictions = loadedPipelineModel.transform(testingData)\n",
        "\n",
        "# Task 4: Mostrar algunas predicciones del modelo cargado\n",
        "print(\"Mostrando predicciones (MPG real vs. Predicción) con modelo cargado:\")\n",
        "loaded_predictions.select(\"MPG\", \"prediction\").show(5)\n",
        "\n",
        "\n",
        "# Evaluación Parte 4 (Adaptada)\n",
        "print(\"\\n--- Evaluación Parte 4 ---\")\n",
        "loaded_lr_model = loadedPipelineModel.stages[-1] # Modelo LR del pipeline cargado\n",
        "totalstages = len(loadedPipelineModel.stages)\n",
        "# Obtenemos los nombres de las columnas de entrada del ensamblador (etapa 1 o índice 1)\n",
        "inputcolumns = loadedPipelineModel.stages[1].getInputCols() # VectorAssembler es stages[1]\n",
        "\n",
        "print(f\"Number of stages in the loaded pipeline = {totalstages}\")\n",
        "print(\"Coefficients from loaded model:\")\n",
        "for feature, coef in zip(inputcolumns, loaded_lr_model.coefficients):\n",
        "    print(f\"  Coefficient for {feature} is {round(coef, 4)}\")\n",
        "\n",
        "print(\"--- Fin Parte 4 ---\")"
      ],
      "metadata": {
        "id": "IGeaoxpfopmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Detener la sesión de Spark para liberar recursos\n",
        "spark.stop()\n",
        "print(\"Spark Session detenida.\")"
      ],
      "metadata": {
        "id": "9KV7a-TqouOq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
